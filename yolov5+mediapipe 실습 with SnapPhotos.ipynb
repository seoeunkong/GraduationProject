{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d66ca44c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\users\\seoeu\\anaconda3\\envs\\yolov5\\lib\\site-packages (4.5.5)\n",
      "Requirement already satisfied: mediapipe in c:\\users\\seoeu\\anaconda3\\envs\\yolov5\\lib\\site-packages (0.8.10)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\seoeu\\anaconda3\\envs\\yolov5\\lib\\site-packages (3.5.1)\n",
      "Requirement already satisfied: absl-py in c:\\users\\seoeu\\anaconda3\\envs\\yolov5\\lib\\site-packages (from mediapipe) (1.0.0)\n",
      "Requirement already satisfied: attrs>=19.1.0 in c:\\users\\seoeu\\anaconda3\\envs\\yolov5\\lib\\site-packages (from mediapipe) (21.4.0)\n",
      "Requirement already satisfied: opencv-contrib-python in c:\\users\\seoeu\\anaconda3\\envs\\yolov5\\lib\\site-packages (from mediapipe) (4.5.5.64)\n",
      "Requirement already satisfied: protobuf>=3.11.4 in c:\\users\\seoeu\\anaconda3\\envs\\yolov5\\lib\\site-packages (from mediapipe) (3.20.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\seoeu\\anaconda3\\envs\\yolov5\\lib\\site-packages (from mediapipe) (1.21.5)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\seoeu\\anaconda3\\envs\\yolov5\\lib\\site-packages (from matplotlib) (9.0.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\seoeu\\anaconda3\\envs\\yolov5\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\seoeu\\anaconda3\\envs\\yolov5\\lib\\site-packages (from matplotlib) (21.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\seoeu\\anaconda3\\envs\\yolov5\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\seoeu\\anaconda3\\envs\\yolov5\\lib\\site-packages (from matplotlib) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\seoeu\\anaconda3\\envs\\yolov5\\lib\\site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\seoeu\\anaconda3\\envs\\yolov5\\lib\\site-packages (from matplotlib) (3.0.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\seoeu\\anaconda3\\envs\\yolov5\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python mediapipe matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63c3e989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mediapipe in c:\\users\\seoeu\\anaconda3\\envs\\yolov5\\lib\\site-packages (0.8.10)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\seoeu\\anaconda3\\envs\\yolov5\\lib\\site-packages (4.5.5)\n",
      "Requirement already satisfied: pandas in c:\\users\\seoeu\\anaconda3\\envs\\yolov5\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\seoeu\\anaconda3\\envs\\yolov5\\lib\\site-packages (1.1.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\seoeu\\anaconda3\\envs\\yolov5\\lib\\site-packages (from mediapipe) (3.5.1)\n",
      "Requirement already satisfied: attrs>=19.1.0 in c:\\users\\seoeu\\anaconda3\\envs\\yolov5\\lib\\site-packages (from mediapipe) (21.4.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\seoeu\\anaconda3\\envs\\yolov5\\lib\\site-packages (from mediapipe) (1.21.5)\n",
      "Requirement already satisfied: opencv-contrib-python in c:\\users\\seoeu\\anaconda3\\envs\\yolov5\\lib\\site-packages (from mediapipe) (4.5.5.64)\n",
      "Requirement already satisfied: absl-py in c:\\users\\seoeu\\anaconda3\\envs\\yolov5\\lib\\site-packages (from mediapipe) (1.0.0)\n",
      "Requirement already satisfied: protobuf>=3.11.4 in c:\\users\\seoeu\\anaconda3\\envs\\yolov5\\lib\\site-packages (from mediapipe) (3.20.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\seoeu\\anaconda3\\envs\\yolov5\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\seoeu\\anaconda3\\envs\\yolov5\\lib\\site-packages (from pandas) (2021.3)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\seoeu\\anaconda3\\envs\\yolov5\\lib\\site-packages (from scikit-learn) (1.7.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\seoeu\\anaconda3\\envs\\yolov5\\lib\\site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: joblib>=1.0.0 in c:\\users\\seoeu\\anaconda3\\envs\\yolov5\\lib\\site-packages (from scikit-learn) (1.1.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\seoeu\\anaconda3\\envs\\yolov5\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\seoeu\\anaconda3\\envs\\yolov5\\lib\\site-packages (from matplotlib->mediapipe) (3.0.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\seoeu\\anaconda3\\envs\\yolov5\\lib\\site-packages (from matplotlib->mediapipe) (21.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\seoeu\\anaconda3\\envs\\yolov5\\lib\\site-packages (from matplotlib->mediapipe) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\seoeu\\anaconda3\\envs\\yolov5\\lib\\site-packages (from matplotlib->mediapipe) (1.3.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\seoeu\\anaconda3\\envs\\yolov5\\lib\\site-packages (from matplotlib->mediapipe) (9.0.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\seoeu\\anaconda3\\envs\\yolov5\\lib\\site-packages (from matplotlib->mediapipe) (4.25.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install mediapipe opencv-python pandas scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10f48797",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "import mediapipe as mp\n",
    "from mediapipe.python.solutions import holistic as mp_holistic\n",
    "\n",
    "# PyTorch Hub\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef4683e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\seoeu/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2022-5-17 Python-3.8.13 torch-1.11.0 CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "# 모델(yolov5) 가져오기\n",
    "yolo_model = torch.hub.load('ultralytics/yolov5', 'yolov5s')\n",
    "\n",
    "# 우리는 사람을 구분하는 것에 관심이 있으니깐\n",
    "yolo_model.classes=[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a309de77",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils\n",
    "# 원래 우리 코드에서는 mp_holistic을 사용함\n",
    "# mp_pose를 mp_holistic으로 바꿔서 해봐야겠다\n",
    "mp_holistic =mp.solutions.holistic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ff1a1d",
   "metadata": {},
   "source": [
    "1. MAKE DETECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b374b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#video_path =\"walking1.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "baf69c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(640, 480)\n"
     ]
    }
   ],
   "source": [
    "# 비디오의 면적 구하기\n",
    "cap = cv2.VideoCapture(0)\n",
    "#cap = cv2.VideoCapture(video_path)\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    h, w, _ = frame.shape\n",
    "    size = (w, h)\n",
    "    print(size)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72bea76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "# Initiate holistic model\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        # Recolor Feed\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False        \n",
    "        \n",
    "        # Make Detections\n",
    "        results = holistic.process(image)\n",
    "        # print(results.face_landmarks)\n",
    "        \n",
    "        # face_landmarks, pose_landmarks, left_hand_landmarks, right_hand_landmarks\n",
    "        \n",
    "        # Recolor image back to BGR for rendering\n",
    "        image.flags.writeable = True   \n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        \n",
    "        # 2. Right hand\n",
    "        mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                                 mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4),\n",
    "                                 mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                                 )\n",
    "\n",
    "        # 3. Left Hand\n",
    "        mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                                 mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4),\n",
    "                                 mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                                 )\n",
    "\n",
    "        # 4. Pose Detections\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS, \n",
    "                                 mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4),\n",
    "                                 mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                                 )\n",
    "                        \n",
    "        cv2.imshow('Raw Webcam Feed', image)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0710635",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9983773827552795"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 이건 pose_landmarks에만 있는 기능. visibility\n",
    "# 해당 랜드마크가 스크린에 보이는지 알려줌\n",
    "results.pose_landmarks.landmark[0].visibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65cfe9aa",
   "metadata": {},
   "source": [
    "추가_ 데이터셋 다시 만들어서 해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19ebd968",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f252740e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(results.face_landmarks.landmark)\n",
    "# len(results.pose_landmarks.landmark)\n",
    "num_coords = len(results.pose_landmarks.landmark)\n",
    "num_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d66fef5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "landmarks = ['class']\n",
    "for val in range(1, num_coords + 1):\n",
    "    landmarks += ['x{}'.format(val), 'y{}'.format(val), 'z{}'.format(val), 'v{}'.format(val)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d509a0da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['class',\n",
       " 'x1',\n",
       " 'y1',\n",
       " 'z1',\n",
       " 'v1',\n",
       " 'x2',\n",
       " 'y2',\n",
       " 'z2',\n",
       " 'v2',\n",
       " 'x3',\n",
       " 'y3',\n",
       " 'z3',\n",
       " 'v3',\n",
       " 'x4',\n",
       " 'y4',\n",
       " 'z4',\n",
       " 'v4',\n",
       " 'x5',\n",
       " 'y5',\n",
       " 'z5',\n",
       " 'v5',\n",
       " 'x6',\n",
       " 'y6',\n",
       " 'z6',\n",
       " 'v6',\n",
       " 'x7',\n",
       " 'y7',\n",
       " 'z7',\n",
       " 'v7',\n",
       " 'x8',\n",
       " 'y8',\n",
       " 'z8',\n",
       " 'v8',\n",
       " 'x9',\n",
       " 'y9',\n",
       " 'z9',\n",
       " 'v9',\n",
       " 'x10',\n",
       " 'y10',\n",
       " 'z10',\n",
       " 'v10',\n",
       " 'x11',\n",
       " 'y11',\n",
       " 'z11',\n",
       " 'v11',\n",
       " 'x12',\n",
       " 'y12',\n",
       " 'z12',\n",
       " 'v12',\n",
       " 'x13',\n",
       " 'y13',\n",
       " 'z13',\n",
       " 'v13',\n",
       " 'x14',\n",
       " 'y14',\n",
       " 'z14',\n",
       " 'v14',\n",
       " 'x15',\n",
       " 'y15',\n",
       " 'z15',\n",
       " 'v15',\n",
       " 'x16',\n",
       " 'y16',\n",
       " 'z16',\n",
       " 'v16',\n",
       " 'x17',\n",
       " 'y17',\n",
       " 'z17',\n",
       " 'v17',\n",
       " 'x18',\n",
       " 'y18',\n",
       " 'z18',\n",
       " 'v18',\n",
       " 'x19',\n",
       " 'y19',\n",
       " 'z19',\n",
       " 'v19',\n",
       " 'x20',\n",
       " 'y20',\n",
       " 'z20',\n",
       " 'v20',\n",
       " 'x21',\n",
       " 'y21',\n",
       " 'z21',\n",
       " 'v21',\n",
       " 'x22',\n",
       " 'y22',\n",
       " 'z22',\n",
       " 'v22',\n",
       " 'x23',\n",
       " 'y23',\n",
       " 'z23',\n",
       " 'v23',\n",
       " 'x24',\n",
       " 'y24',\n",
       " 'z24',\n",
       " 'v24',\n",
       " 'x25',\n",
       " 'y25',\n",
       " 'z25',\n",
       " 'v25',\n",
       " 'x26',\n",
       " 'y26',\n",
       " 'z26',\n",
       " 'v26',\n",
       " 'x27',\n",
       " 'y27',\n",
       " 'z27',\n",
       " 'v27',\n",
       " 'x28',\n",
       " 'y28',\n",
       " 'z28',\n",
       " 'v28',\n",
       " 'x29',\n",
       " 'y29',\n",
       " 'z29',\n",
       " 'v29',\n",
       " 'x30',\n",
       " 'y30',\n",
       " 'z30',\n",
       " 'v30',\n",
       " 'x31',\n",
       " 'y31',\n",
       " 'z31',\n",
       " 'v31',\n",
       " 'x32',\n",
       " 'y32',\n",
       " 'z32',\n",
       " 'v32',\n",
       " 'x33',\n",
       " 'y33',\n",
       " 'z33',\n",
       " 'v33']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# landmarks[-3]\n",
    "landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "fc3db140",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('coords.csv', mode = 'w', newline = '') as f:\n",
    "    csv_writer = csv.writer(f, delimiter = ',', quotechar = '\"', quoting = csv.QUOTE_MINIMAL)\n",
    "    csv_writer.writerow(landmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c5d4bf68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_name = \"Kicking_R\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1124e0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "# Initiate holistic model\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        # Recolor Feed\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False        \n",
    "        \n",
    "        # Make Detections\n",
    "        results = holistic.process(image)\n",
    "        # print(results.face_landmarks)\n",
    "        \n",
    "        # face_landmarks, pose_landmarks, left_hand_landmarks, right_hand_landmarks\n",
    "        \n",
    "        # Recolor image back to BGR for rendering\n",
    "        image.flags.writeable = True   \n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        \n",
    "        # 2. Right hand\n",
    "        mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                                 mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4),\n",
    "                                 mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                                 )\n",
    "\n",
    "        # 3. Left Hand\n",
    "        mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                                 mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4),\n",
    "                                 mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                                 )\n",
    "\n",
    "        # 4. Pose Detections\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS, \n",
    "                                 mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4),\n",
    "                                 mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                                 )\n",
    "        \n",
    "        # Export coordinates\n",
    "        try:\n",
    "            # Extract Pose landmarks\n",
    "            pose = results.pose_landmarks.landmark\n",
    "            pose_row = list(np.array([[landmark.x, landmark.y, landmark.z, landmark.visibility] for landmark in pose]).flatten())\n",
    "            \n",
    "            # Extract Face landmarks\n",
    "            \n",
    "            \n",
    "            # Concate rows\n",
    "            row = pose_row\n",
    "            #+face_row\n",
    "            \n",
    "            # Append class name \n",
    "            row.insert(0, class_name)\n",
    "            \n",
    "            # Export to CSV\n",
    "            with open('coords.csv', mode='a', newline='') as f:\n",
    "                csv_writer = csv.writer(f, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "                csv_writer.writerow(row) \n",
    "            \n",
    "        except:\n",
    "            pass\n",
    "                        \n",
    "        cv2.imshow('Raw Webcam Feed', image)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "9e023ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pose = results.pose_landmarks.landmark\n",
    "# face = results.face_landmarks.landmark\n",
    "pose_row = list(np.array([[landmark.x, landmark.y, landmark.z, landmark.visibility] for landmark in pose]).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "350d3495",
   "metadata": {},
   "outputs": [],
   "source": [
    "row = pose_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a42deacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "row.insert(0, class_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "62e98e3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Kicking_R',\n",
       " 0.6698282361030579,\n",
       " 0.16727600991725922,\n",
       " -2.084416151046753,\n",
       " 0.9975929856300354,\n",
       " 0.7230377793312073,\n",
       " 0.08516144752502441,\n",
       " -2.0149152278900146,\n",
       " 0.9929424524307251,\n",
       " 0.7541576623916626,\n",
       " 0.0891655683517456,\n",
       " -2.015479803085327,\n",
       " 0.9919734001159668,\n",
       " 0.7867369651794434,\n",
       " 0.09572768211364746,\n",
       " -2.0161173343658447,\n",
       " 0.9900604486465454,\n",
       " 0.6407448053359985,\n",
       " 0.08705508708953857,\n",
       " -1.9786193370819092,\n",
       " 0.9945776462554932,\n",
       " 0.6139675378799438,\n",
       " 0.09165358543395996,\n",
       " -1.978209137916565,\n",
       " 0.9953285455703735,\n",
       " 0.5877035856246948,\n",
       " 0.09662461280822754,\n",
       " -1.9796736240386963,\n",
       " 0.9960290193557739,\n",
       " 0.8540937304496765,\n",
       " 0.14440488815307617,\n",
       " -1.3474327325820923,\n",
       " 0.9876340627670288,\n",
       " 0.5541505813598633,\n",
       " 0.13872480392456055,\n",
       " -1.141118049621582,\n",
       " 0.997208297252655,\n",
       " 0.7229864001274109,\n",
       " 0.26061999797821045,\n",
       " -1.8233051300048828,\n",
       " 0.9988704323768616,\n",
       " 0.616604745388031,\n",
       " 0.2567373514175415,\n",
       " -1.763339877128601,\n",
       " 0.9992820620536804,\n",
       " 1.0514397621154785,\n",
       " 0.5457035303115845,\n",
       " -0.8475191593170166,\n",
       " 0.9875516295433044,\n",
       " 0.36260610818862915,\n",
       " 0.5161608457565308,\n",
       " -0.7118017077445984,\n",
       " 0.9990472197532654,\n",
       " 1.2132997512817383,\n",
       " 1.0824536085128784,\n",
       " -0.7464166283607483,\n",
       " 0.2872588336467743,\n",
       " 0.14106863737106323,\n",
       " 0.9038870930671692,\n",
       " -0.5183421969413757,\n",
       " 0.7432630062103271,\n",
       " 1.148041009902954,\n",
       " 1.4964920282363892,\n",
       " -1.4876230955123901,\n",
       " 0.026046661660075188,\n",
       " 0.0242803692817688,\n",
       " 1.3931269645690918,\n",
       " -1.3156617879867554,\n",
       " 0.3121070861816406,\n",
       " 1.1692137718200684,\n",
       " 1.6469824314117432,\n",
       " -1.7252724170684814,\n",
       " 0.031184349209070206,\n",
       " -0.03664136305451393,\n",
       " 1.5349817276000977,\n",
       " -1.5531078577041626,\n",
       " 0.2608978748321533,\n",
       " 1.0958589315414429,\n",
       " 1.6353298425674438,\n",
       " -1.8545321226119995,\n",
       " 0.0478847436606884,\n",
       " 0.03564668074250221,\n",
       " 1.5268723964691162,\n",
       " -1.7587394714355469,\n",
       " 0.35163602232933044,\n",
       " 1.0694835186004639,\n",
       " 1.5719305276870728,\n",
       " -1.5747114419937134,\n",
       " 0.0484822653234005,\n",
       " 0.0753103494644165,\n",
       " 1.480224370956421,\n",
       " -1.4273405075073242,\n",
       " 0.36335405707359314,\n",
       " 0.8955743312835693,\n",
       " 1.4955308437347412,\n",
       " -0.18091772496700287,\n",
       " 0.0013296606484800577,\n",
       " 0.4008120894432068,\n",
       " 1.4565376043319702,\n",
       " 0.18669824302196503,\n",
       " 0.0021094258408993483,\n",
       " 0.8080522418022156,\n",
       " 2.274498462677002,\n",
       " 0.26993316411972046,\n",
       " 0.0013991785235702991,\n",
       " 0.35394051671028137,\n",
       " 2.241288185119629,\n",
       " 0.6031396985054016,\n",
       " 0.0007580145611427724,\n",
       " 0.7628979086875916,\n",
       " 2.9459564685821533,\n",
       " 1.828231930732727,\n",
       " 0.0003669650759547949,\n",
       " 0.3377862870693207,\n",
       " 2.917065382003784,\n",
       " 1.9379733800888062,\n",
       " 7.522324449382722e-05,\n",
       " 0.7686857581138611,\n",
       " 3.057352066040039,\n",
       " 1.922617793083191,\n",
       " 0.00012622807116713375,\n",
       " 0.33193010091781616,\n",
       " 3.0199098587036133,\n",
       " 2.0471689701080322,\n",
       " 0.00020938199304509908,\n",
       " 0.6741452813148499,\n",
       " 3.16351318359375,\n",
       " 0.8326646685600281,\n",
       " 0.00018539537268225104,\n",
       " 0.3885377049446106,\n",
       " 3.159849166870117,\n",
       " 0.8516779541969299,\n",
       " 0.00024541604216210544]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row\n",
    "# len(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdb09ed",
   "metadata": {},
   "source": [
    "2. READ IN COLLECTED DATA AND PROCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6dd0cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0956cca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('coords0518_S.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ecf49c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>x1</th>\n",
       "      <th>y1</th>\n",
       "      <th>z1</th>\n",
       "      <th>v1</th>\n",
       "      <th>...</th>\n",
       "      <th>v32</th>\n",
       "      <th>x33</th>\n",
       "      <th>y33</th>\n",
       "      <th>z33</th>\n",
       "      <th>v33</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Standing</td>\n",
       "      <td>0.551715</td>\n",
       "      <td>0.411068</td>\n",
       "      <td>-0.480280</td>\n",
       "      <td>0.999614</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035616</td>\n",
       "      <td>0.512582</td>\n",
       "      <td>1.247243</td>\n",
       "      <td>0.246170</td>\n",
       "      <td>0.037159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Standing</td>\n",
       "      <td>0.550886</td>\n",
       "      <td>0.411065</td>\n",
       "      <td>-0.501023</td>\n",
       "      <td>0.999620</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033711</td>\n",
       "      <td>0.513790</td>\n",
       "      <td>1.278817</td>\n",
       "      <td>0.316399</td>\n",
       "      <td>0.035369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Standing</td>\n",
       "      <td>0.550391</td>\n",
       "      <td>0.410203</td>\n",
       "      <td>-0.497141</td>\n",
       "      <td>0.999628</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031328</td>\n",
       "      <td>0.514399</td>\n",
       "      <td>1.285124</td>\n",
       "      <td>0.345723</td>\n",
       "      <td>0.033125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Standing</td>\n",
       "      <td>0.550295</td>\n",
       "      <td>0.409257</td>\n",
       "      <td>-0.498234</td>\n",
       "      <td>0.999636</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029196</td>\n",
       "      <td>0.514306</td>\n",
       "      <td>1.289704</td>\n",
       "      <td>0.342759</td>\n",
       "      <td>0.031088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Standing</td>\n",
       "      <td>0.550293</td>\n",
       "      <td>0.408860</td>\n",
       "      <td>-0.496996</td>\n",
       "      <td>0.999631</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027317</td>\n",
       "      <td>0.514810</td>\n",
       "      <td>1.290418</td>\n",
       "      <td>0.330170</td>\n",
       "      <td>0.029151</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 133 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      class        x1        y1        z1        v1  ...       v32       x33  \\\n",
       "0  Standing  0.551715  0.411068 -0.480280  0.999614  ...  0.035616  0.512582   \n",
       "1  Standing  0.550886  0.411065 -0.501023  0.999620  ...  0.033711  0.513790   \n",
       "2  Standing  0.550391  0.410203 -0.497141  0.999628  ...  0.031328  0.514399   \n",
       "3  Standing  0.550295  0.409257 -0.498234  0.999636  ...  0.029196  0.514306   \n",
       "4  Standing  0.550293  0.408860 -0.496996  0.999631  ...  0.027317  0.514810   \n",
       "\n",
       "        y33       z33       v33  \n",
       "0  1.247243  0.246170  0.037159  \n",
       "1  1.278817  0.316399  0.035369  \n",
       "2  1.285124  0.345723  0.033125  \n",
       "3  1.289704  0.342759  0.031088  \n",
       "4  1.290418  0.330170  0.029151  \n",
       "\n",
       "[5 rows x 133 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5bb0a584",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>x1</th>\n",
       "      <th>y1</th>\n",
       "      <th>z1</th>\n",
       "      <th>v1</th>\n",
       "      <th>...</th>\n",
       "      <th>v32</th>\n",
       "      <th>x33</th>\n",
       "      <th>y33</th>\n",
       "      <th>z33</th>\n",
       "      <th>v33</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19466</th>\n",
       "      <td>Kicking_L</td>\n",
       "      <td>0.474280</td>\n",
       "      <td>0.126603</td>\n",
       "      <td>-0.334460</td>\n",
       "      <td>0.999939</td>\n",
       "      <td>...</td>\n",
       "      <td>0.990630</td>\n",
       "      <td>0.498406</td>\n",
       "      <td>0.945859</td>\n",
       "      <td>-0.049017</td>\n",
       "      <td>0.968442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19467</th>\n",
       "      <td>Kicking_L</td>\n",
       "      <td>0.474263</td>\n",
       "      <td>0.125803</td>\n",
       "      <td>-0.335036</td>\n",
       "      <td>0.999939</td>\n",
       "      <td>...</td>\n",
       "      <td>0.990636</td>\n",
       "      <td>0.498432</td>\n",
       "      <td>0.945885</td>\n",
       "      <td>-0.053361</td>\n",
       "      <td>0.968411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19468</th>\n",
       "      <td>Kicking_L</td>\n",
       "      <td>0.474216</td>\n",
       "      <td>0.124637</td>\n",
       "      <td>-0.334989</td>\n",
       "      <td>0.999938</td>\n",
       "      <td>...</td>\n",
       "      <td>0.990511</td>\n",
       "      <td>0.498262</td>\n",
       "      <td>0.945865</td>\n",
       "      <td>-0.050120</td>\n",
       "      <td>0.968360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19469</th>\n",
       "      <td>Kicking_L</td>\n",
       "      <td>0.474388</td>\n",
       "      <td>0.122454</td>\n",
       "      <td>-0.329588</td>\n",
       "      <td>0.999937</td>\n",
       "      <td>...</td>\n",
       "      <td>0.990563</td>\n",
       "      <td>0.498302</td>\n",
       "      <td>0.945868</td>\n",
       "      <td>-0.053111</td>\n",
       "      <td>0.967456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19470</th>\n",
       "      <td>Kicking_L</td>\n",
       "      <td>0.474614</td>\n",
       "      <td>0.120778</td>\n",
       "      <td>-0.329746</td>\n",
       "      <td>0.999936</td>\n",
       "      <td>...</td>\n",
       "      <td>0.990386</td>\n",
       "      <td>0.498667</td>\n",
       "      <td>0.945846</td>\n",
       "      <td>-0.068322</td>\n",
       "      <td>0.966731</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 133 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           class        x1        y1        z1        v1  ...       v32  \\\n",
       "19466  Kicking_L  0.474280  0.126603 -0.334460  0.999939  ...  0.990630   \n",
       "19467  Kicking_L  0.474263  0.125803 -0.335036  0.999939  ...  0.990636   \n",
       "19468  Kicking_L  0.474216  0.124637 -0.334989  0.999938  ...  0.990511   \n",
       "19469  Kicking_L  0.474388  0.122454 -0.329588  0.999937  ...  0.990563   \n",
       "19470  Kicking_L  0.474614  0.120778 -0.329746  0.999936  ...  0.990386   \n",
       "\n",
       "            x33       y33       z33       v33  \n",
       "19466  0.498406  0.945859 -0.049017  0.968442  \n",
       "19467  0.498432  0.945885 -0.053361  0.968411  \n",
       "19468  0.498262  0.945865 -0.050120  0.968360  \n",
       "19469  0.498302  0.945868 -0.053111  0.967456  \n",
       "19470  0.498667  0.945846 -0.068322  0.966731  \n",
       "\n",
       "[5 rows x 133 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c0cbf2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('class', axis=1) # features\n",
    "y = df['class'] # target value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bd692a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, y_train: for fitting/training model\n",
    "# X_test, y_test: for actuallly valuating model\n",
    "# test_size = 0.3  ->  testing partition is 30 percent\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "afb518d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4277     Kicking_R\n",
       "3821     Kicking_R\n",
       "16770     Standing\n",
       "18161    Kicking_L\n",
       "19189    Kicking_L\n",
       "           ...    \n",
       "4481     Kicking_L\n",
       "14845     Standing\n",
       "4982     Kicking_L\n",
       "17818    Kicking_L\n",
       "507       Standing\n",
       "Name: class, Length: 5842, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fighting_r일때 캡처\n",
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3716d8",
   "metadata": {},
   "source": [
    "3. TRAIN MACHINE LEARNING CLASSIFICATION MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "abad9c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for making machine learning pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9fa3db9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelines = {\n",
    "    'lr' : make_pipeline(StandardScaler(), LogisticRegression()),\n",
    "    'rc' : make_pipeline(StandardScaler(), RidgeClassifier()),\n",
    "    'rf' : make_pipeline(StandardScaler(), RandomForestClassifier()),\n",
    "    'gb' : make_pipeline(StandardScaler(), GradientBoostingClassifier()),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "01b36b85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;standardscaler&#x27;, StandardScaler()),\n",
       "                (&#x27;logisticregression&#x27;, LogisticRegression())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;standardscaler&#x27;, StandardScaler()),\n",
       "                (&#x27;logisticregression&#x27;, LogisticRegression())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                ('logisticregression', LogisticRegression())])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pipelines.keys()\n",
    "list(pipelines.values())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1d6131da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seoeu\\anaconda3\\envs\\yolov5\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "fit_models = {}\n",
    "for algo, pipeline in pipelines.items():\n",
    "    model = pipeline.fit(X_train.values, y_train.values)\n",
    "    fit_models[algo] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3d9b9a80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lr': Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                 ('logisticregression', LogisticRegression())]),\n",
       " 'rc': Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                 ('ridgeclassifier', RidgeClassifier())]),\n",
       " 'rf': Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                 ('randomforestclassifier', RandomForestClassifier())]),\n",
       " 'gb': Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                 ('gradientboostingclassifier', GradientBoostingClassifier())])}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "97e8f7be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;standardscaler&#x27;, StandardScaler()),\n",
       "                (&#x27;randomforestclassifier&#x27;, RandomForestClassifier())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;standardscaler&#x27;, StandardScaler()),\n",
       "                (&#x27;randomforestclassifier&#x27;, RandomForestClassifier())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                ('randomforestclassifier', RandomForestClassifier())])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_models['rf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "462ce88f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seoeu\\anaconda3\\envs\\yolov5\\lib\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['Kicking_R', 'Kicking_R', 'Standing', ..., 'Kicking_L', 'Kicking_L', 'Standing'], dtype=object)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_models['lr'].predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a79985",
   "metadata": {},
   "source": [
    "4. EVALUATE AND SERIALIZE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d0942658",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score # Accuracy metrics\n",
    "import pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "31282f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seoeu\\anaconda3\\envs\\yolov5\\lib\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\seoeu\\anaconda3\\envs\\yolov5\\lib\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\seoeu\\anaconda3\\envs\\yolov5\\lib\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr 0.9962341663813763\n",
      "rc 0.9881889763779528\n",
      "rf 0.999828825744608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seoeu\\anaconda3\\envs\\yolov5\\lib\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gb 0.9979459089352961\n"
     ]
    }
   ],
   "source": [
    "for algo, model in fit_models.items():\n",
    "    yhat = model.predict(X_test)\n",
    "    print(algo, accuracy_score(y_test, yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8d839082",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seoeu\\anaconda3\\envs\\yolov5\\lib\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['Kicking_R', 'Kicking_R', 'Standing', ..., 'Kicking_L', 'Kicking_L', 'Standing'], dtype=object)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_models['rf'].predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d0eeafb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4277     Kicking_R\n",
       "3821     Kicking_R\n",
       "16770     Standing\n",
       "18161    Kicking_L\n",
       "19189    Kicking_L\n",
       "           ...    \n",
       "4481     Kicking_L\n",
       "14845     Standing\n",
       "4982     Kicking_L\n",
       "17818    Kicking_L\n",
       "507       Standing\n",
       "Name: class, Length: 5842, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eacdf9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('body_language.pkl', 'wb') as f:\n",
    "    pickle.dump(fit_models['rf'], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58408fc",
   "metadata": {},
   "source": [
    "5. MAKE DETECTIONS WITH MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f35d10dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model loaded\n",
    "with open('body_language.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1545264e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;standardscaler&#x27;, StandardScaler()),\n",
       "                (&#x27;randomforestclassifier&#x27;, RandomForestClassifier())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;standardscaler&#x27;, StandardScaler()),\n",
       "                (&#x27;randomforestclassifier&#x27;, RandomForestClassifier())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                ('randomforestclassifier', RandomForestClassifier())])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e31ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 카메라 연결. 맥에서는 숫자 2로 하는게 낫대\n",
    "cap = cv2.VideoCapture(0, cv2.CAP_DSHOW)\n",
    "#cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# 웹캠 프레임 단위로 돌린다\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "        \n",
    "    # Recolor Feed \n",
    "    image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False\n",
    "        \n",
    "    # Make Detections\n",
    "    result = yolo_model(image)\n",
    "    # print(results.face_Landmarks)\n",
    "    # face_landmarks, pose_landmarks, Left_hand_landmarks, right_hand_landmarks   \n",
    "    # Recolor image back to BGR for rendering\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    img_list = []\n",
    "    \n",
    "    MARGIN = 10\n",
    "    \n",
    "    for (xmin, ymin, xmax, ymax, confidence, clas) in result.xyxy[0].tolist():\n",
    "        # Initiate holistic model\n",
    "      with mp_holistic.Holistic(min_detection_confidence=0.3, min_tracking_confidence=0.3) as holistic:\n",
    "        # 이게 사람 구분하는 코드\n",
    "        #Media pose prediction ,we are \n",
    "        results = holistic.process(image[int(ymin)+MARGIN:int(ymax)+MARGIN,int(xmin)+MARGIN:int(xmax)+MARGIN:])\n",
    "\n",
    "        #Draw landmarks on image, if this thing is confusing please consider going through numpy array slicing \n",
    "        # 2. Right hand\n",
    "        mp_drawing.draw_landmarks(image[int(ymin)+MARGIN:int(ymax)+MARGIN,int(xmin)+MARGIN:int(xmax)+MARGIN:], results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                                  mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=2), \n",
    "                                  mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2) \n",
    "                                 )\n",
    "        # 3. Left hand\n",
    "        mp_drawing.draw_landmarks(image[int(ymin)+MARGIN:int(ymax)+MARGIN,int(xmin)+MARGIN:int(xmax)+MARGIN:], results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                                  mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=2), \n",
    "                                  mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2) \n",
    "                                 )\n",
    "        # 4. Pose Detections\n",
    "        mp_drawing.draw_landmarks(image[int(ymin)+MARGIN:int(ymax)+MARGIN,int(xmin)+MARGIN:int(xmax)+MARGIN:], results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                                  mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=2), \n",
    "                                  mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2) \n",
    "                                 )\n",
    "        img_list.append(image[int(ymin):int(ymax),int(xmin):int(xmax):])\n",
    "        \n",
    "        # Happy를 알려주는 데이터 추출(우리가 웃는 표정을 데이터로 뽑는 것)\n",
    "        # Export coordinates\n",
    "        try:\n",
    "            # Extract Pose landmarks\n",
    "            pose = results.pose_landmarks.landmark\n",
    "            pose_row = list(np.array([[landmark.x, landmark.y, landmark.z, landmark.visibility] for landmark in pose]).flatten())\n",
    "\n",
    "            # Concate rows\n",
    "            row = pose_row\n",
    "\n",
    "            # Make Detections\n",
    "            X = pd.DataFrame([row])\n",
    "            body_language_class = model.predict(X)[0]\n",
    "            body_language_prob = model.predict_proba(X)[0]\n",
    "            #print(body_language_class, body_language_prob)\n",
    "            \n",
    "            # Grab ear coords\n",
    "            coords = tuple(np.multiply(\n",
    "                            np.array(\n",
    "                                (results.pose_landmarks.landmark[mp_holistic.PoseLandmark.NOSE].x,\n",
    "                                 results.pose_landmarks.landmark[mp_holistic.PoseLandmark.NOSE].y))\n",
    "                        , [640,480]).astype(int))\n",
    "            cv2.rectangle(image, (coords[0], coords[1] + 5), (coords[0] + len(body_language_class) * 20, coords[1] - 30), (245, 117, 16), -1)\n",
    "            #print(\"coords[0]: \",coords[0],\", coords[1]: \",coords[1])\n",
    "           \n",
    "            \n",
    "            cv2.putText(image, body_language_class, coords,cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "           \n",
    "            \n",
    "            \"\"\"\n",
    "            #Get status box\n",
    "            # 첫 번째 인수: 시각형 시작 좌표, 두 번째: 사각형 크기\n",
    "            cv2.rectangle(image, (0, 0), (300, 60), (245, 117, 16), -1)\n",
    "            \n",
    "            \n",
    "            # Display Class\n",
    "            cv2.putText(image, 'CLASS'\n",
    "                       , (125, 12), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "            cv2.putText(image, body_language_class.split(' ')[0]\n",
    "                       , (120, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "           \n",
    "            \n",
    "            # Display Probability\n",
    "            cv2.putText(image, 'PROB'\n",
    "                       , (15, 12), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "            cv2.putText(image, str(round(body_language_prob[np.argmax(body_language_prob)], 2))\n",
    "                       , (10, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "            \"\"\"\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    cv2.imshow('Raw Webcam Feed', image)\n",
    "    \n",
    "    \n",
    "    if body_language_class == \"Fighting_R\":\n",
    "        tsec = time.localtime(time.time())\n",
    "        tname = time.strftime(\"%Y-%m-%d %Hh%Mm%Ss\", tsec)\n",
    "        title = 'FightingR_Snap_'+tname+'.jpg'\n",
    "        cv2.imwrite(title,image)\n",
    "       \n",
    "        \n",
    "        \n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        break\n",
    "            \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121c0459",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
